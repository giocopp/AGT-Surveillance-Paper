\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Page geometry
\geometry{
    left=1in,
    right=1in,
    top=1in,
    bottom=1in
}

% Line spacing
\onehalfspacing

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\textit{Algorithmic Game Theory and Governance}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}

% Theorem environments
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

% Title Page
\begin{titlepage}
    \thispagestyle{empty}
    
    \vspace*{2cm}
    \begin{center}
    
    {\Large\bfseries Algorithmic Surveillance and Population Control of Palestinians}\\[0.35cm]
    {\large\bfseries A Game-Theoretic Analysis of Israel's Blue Wolf System in Hebron}\\[1.2cm]
    
    {\normalsize Giorgio Coppola and Giulia Maria Petrilli}\\[0.6cm]
    
    {\small
    GRAD-E1489: Algorithmic Game Theory and Governance\\
    Hertie School\\
    Fall 2025
    }
    
    \end{center}
    \vspace*{2cm}
    
    \begin{minipage}{0.9\textwidth}
    \small
    \begin{center}
    \textbf{Abstract}
    \end{center}
    \noindent
    This paper models algorithmic surveillance as a strategic game of incomplete information,
    using Israel's Blue Wolf biometric identification program in Hebron as a case study. Building on the preventive 
    repression framework of \citet{dragu2021digital}, we examine how algorithmic classification and risk scoring create
    strategic dynamics between state surveillance apparatus and targeted populations. The analysis suggests a mechanism through which
    technological advances in surveillance can be associated with systematically harsher control policies. 
    We introduce an interpretation suggesting that algorithmic errors and false positives may be tolerated or even strategically 
    advantageous under certain political objectives. The paper contributes to debates on algorithmic governance by demonstrating 
    how formal game theory can illuminate the political logic of surveillance systems.

    \vspace{1cm}
    \noindent\textbf{Keywords:} Algorithmic surveillance; game theory; incomplete information; Israel--Palestine; biometric identification; 
    preventive repression
    \end{minipage}
    
    \vspace*{1.5cm}
    \end{titlepage}    

% 1. INTRODUCTION
\section{Introduction}
Algorithmic systems aimed at classifying, tracking, and predicting the behavior of populations are deeply debated in democracies, but they are already 
in place where people have fewer freedoms. When used by authoritarian governments toward populations whose rights are limited,
such technology creates strategic environments. Risk scoring, facial recognition, and movement prediction technologies create continuous games 
between surveillance authorities and those subjected to monitoring. In fact, the latter might choose to adapt their behavior to defy such 
systems, and in turn, the surveillance authorities may be able to learn from adaptation and change their strategy accordingly. 
Therefore, this paper asks: How do targets of algorithmic surveillance adapt their behavior? Does this strategic interaction push systems toward 
harsher policies and/or systematic errors? And crucially, are such errors tolerated or even instrumentalized because they serve broader goals 
of population control? We address these questions through analysis of Israel's Blue Wolf system, a biometric surveillance program deployed by the 
Israeli Defense Forces (IDF) in the occupied West Bank, with particular intensity in Hebron. Blue Wolf combines facial recognition technology, 
an extensive biographical database (Wolf Pack), and a color-coded risk classification system to allow instantaneous identification and sorting 
of Palestinian civilians \citep{dwoskin2021israel, amnesty2023automated}. Israeli authorities have framed these technologies as fundamental for 
both enabling a ``frictionless'' security policy, reducing invasive checkpoints while maintaining security control, and for facilitating the 
occupation, defining the system as ``the Facebook of Palestinians'' and Hebron as a ``Smart City,'' highlighting their dual logic of 
surveillance \citep{dwoskin2021israel}. According to experts, Israel's use of facial recognition is often described as among the most advanced 
deployments of such technology by a country seeking to control a subject population \citep{fatafta2017surveillance}. Ethnographic research reveals 
that Blue Wolf intensifies rather than reduces the violence of occupation, eroding Palestinian social life and private space 
\citep{goodfriend2023algorithmic}. It is important to note that, even if Israel is commonly classified as a democracy in terms of electoral 
institutions, a substantial literature argues that its governance toward Palestinians is characterized by stratified citizenship and coercive 
control practices consistent with illiberal or authoritarian logics 
\citep{dayan2022dualstate,gidron2023crisis,jamal2007hollow,smooha2002ethnic,rouhana1997identities,peled2008evolution}.

This work develops a game-theoretic framework to analyze the strategic logic underlying such systems. Building on \citet{dragu2021digital}'s 
model of digital authoritarianism and preventive repression, we extend their analysis to incorporate algorithmic classification as probabilistic 
identification, systematic errors, and behavioral adaptation by surveilled populations. In our static setting, the model highlights an escalation 
mechanism: improvements in surveillance capacity raise the incentives to monitor, while simultaneously inducing costly adaptation by the targeted 
population, sustaining pressure toward expanded control even when the prevalence of genuine threats is low. The paper proceeds as follows. 
Section 2 situates the analysis within the relevant literature. 
Section 3 presents the empirical context of Blue Wolf in Hebron. Section 4 develops the formal model. Section 5 analyzes equilibrium behavior 
and comparative statics. Section 6 discusses policy implications and limitations. Section 7 identifies a venue for this case to be modeled as 
a signaling game, providing direction for further research. 

% 2. THEORETICAL FRAMEWORK
\section{Theoretical Framework}

\citet{dragu2021digital} provide the foundational framework for this analysis. They model the strategic interaction between an authoritarian 
government engaging in preventive repression and an opposition group attempting to mobilize dissent. Their key insight is that digital 
technology has dual effects: it lowers both the barriers to state surveillance and the barriers to opposition mobilization. Within this strategic 
context, they demonstrate that technological advancement consistently increases equilibrium levels of preventive repression, even when technology 
also benefits opposition groups. Three results from their analysis are particularly relevant. First, technological innovation unconditionally 
increases government preventive repression in equilibrium. Second, the probability that government successfully prevents opposition mobilization 
increases with technological development. Third, authoritarian governments prefer technologies that reduce the cost of preventive control and
will strategically allow or block technologies based on their differential effects. We extend this framework in two directions. First, we 
incorporate incomplete information about individual `threat types,' i.e., the fact that the state cannot perfectly distinguish between those who 
pose real security threats and ordinary civilians. This generates classification errors with strategic consequences. Second, we model behavioral 
adaptation by surveilled populations, i.e., changes in movement, communication, and social behavior in response to algorithmic monitoring. 

Critical surveillance studies emphasize that algorithmic systems do not simply observe populations but actively constitute them as objects of 
governance \citep{browne2015dark, benjamin2019race}. Risk scoring and predictive analytics impose categories that shape subsequent treatment, 
creating what \citet{harcourt2007against} terms `actuarial justice.' In contexts of colonial rule, such systems articulate with longer histories 
of population management and territorial control \citep{zureik2011colonialism, shalhoub2015security}. \citet{byler2022terror}'s concept of 
`terror capitalism' illuminates how surveillance technologies serve dual functions: extracting value (valuable information) through data 
accumulation while facilitating state control over minoritized populations. In the Palestinian context, \citet{goodfriend2023algorithmic} 
documents how Blue Wolf is functional to the simultaneous intensification of control over Palestinians and dispossession of their land and 
resources, while enabling capital accumulation within Israeli society. Additionally, studies suggest that algorithmic 
`errors,' such as misclassifications, may not simply be technical failures to be minimized, but may serve political functions within regimes of population 
control. Investigative journalism has clarified that in the context of the search and elimination of targets, the 
Israeli army has dramatically expanded its definition of ``human target'' since October 7th, when Hamas-led militants launched a deadly assault 
on southern Israeli communities \citep{abraham2024lavender}. From being a term used to designate senior military operatives, under ``Operation 
Iron Swords,'' the army started to designate all operatives of Hamas's military wing as human targets, irrespective of their rank or military 
importance. Being now the number of targets in the thousands, the IDF recurred to the use of a new artificial intelligence system, called 
Lavender, to add people to their `kill list.' The approval to automatically adopt Lavender's kill lists was only granted about two weeks into 
the war, even if the program is relatively highly prone to errors and misclassifications \citep{abraham2024lavender}. This precedent gives reasons to 
investigate further whether other systems used for population control might have similar faults. Additionally, detecting many real security 
threats would reinforce the counter-terrorism frame that benefits the political goals of Israel and its allies, which suggests that a 
misclassification might not only be unaccounted for, but even desirable. The literature gives reasons to believe that misclassification is not 
fully disciplined by institutional incentives and may persist because it may align with broader political interests. The game-theoretic 
framework developed below formalizes this intuition.

% 3. EMPIRICAL CONTEXT
\section{Empirical Context: Blue Wolf in Hebron}

Blue Wolf is a mobile application deployed by the IDF that allows soldiers to identify Palestinians through facial recognition and biometric 
matching. The system connects to Wolf Pack, a database containing biographical information, family histories, employment records, and assigned 
security ratings for Palestinians across the West Bank. The program is paralleled by Red Wolf, which extends this system to automated facial recognition 
at checkpoints in the West Bank. In the context of Blue Wolf, when a soldier photographs a Palestinian, the app retrieves their profile and displays a 
color-coded risk assessment: 
red (detain), yellow (delay for further questioning), or green (release). The system's development relied on intensive data collection. Soldiers were 
assigned quotas, reportedly fifty photographs per shift, and competed for prizes based on the number of `pairings' achieved \citep{kubovich2022israeli}. 
This gamification of surveillance \citep{benjamin2019playing} transformed military patrols into data harvesting operations, with Palestinian civilians 
serving as data sources for algorithmic refinement.

Ethnographic research documents how Blue Wolf transforms everyday life in Hebron: movement through the Old City becomes an ongoing encounter 
with checkpoints, cameras, and soldiers equipped with biometric devices, prompting residents to self-police their mobility and domestic routines. 
Interviewees describe avoiding monitored thoroughfares and contending with unpredictable stops and scans; at home, surveillance infrastructures 
installed on rooftops and the possibility of unannounced soldier entry reshape intimate life. Some residents report staying inside to be present 
when soldiers arrive for maintenance, keeping hijabs and ``outside'' garments on indoors, and even abandoning patios or other semi-private spaces 
that now feel exposed to cameras. Attempts to create visual barriers (e.g., hanging tarps) are quickly dismantled, underscoring how adaptation 
is both pervasive and constrained under algorithmic monitoring \citep{goodfriend2023algorithmic}. Critically, the system's errors generate 
significant consequences. Such errors, rather than undermining the system's legitimacy, appear absorbed into its operation. The possibility of 
misclassification may become a mechanism of control, as residents must assume any interaction with soldiers could result in detention regardless 
of actual behavior. 

% 4. THE MODEL
\section{The Model}

\subsection{Setup}

We model the interaction between a surveillance authority (the Israeli military, denoted $G$) and a population of civilians subject to 
monitoring (Palestinians in Hebron, denoted $P$). Following \citet{dragu2021digital}, this is a simultaneous-move game where $G$ chooses a 
level of surveillance effort and $P$ chooses a level of behavioral adaptation.

\textbf{Players and Types.} The population $P$ consists of a set of individuals. Each individual $i$ has a private type 
$\theta_i \in \{T, N\}$, where $T$ denotes `threat' (e.g., engaged in violent resistance activities) and $N$ denotes `non-threat' 
(ordinary civilian, e.g., possibly engaged in social and political activities). Let $\lambda \in (0,1)$ denote the prior probability that a 
randomly selected individual is type $T$. This prior is common knowledge, but individual types are private information.

\textbf{Government Action.} The government chooses surveillance intensity $s \in [0, \bar{s}]$, which determines the probability of detecting 
and correctly classifying individuals. Higher $s$ increases both the probability of identifying actual threats and the probability of false 
positives among non-threats. Let:
\begin{itemize}[nosep]
    \item $\pi_T(\cdot)$ = probability of correctly identifying a type $T$ individual (true positive rate)
    \item $\pi_N(\cdot)$ = probability of incorrectly flagging a type $N$ individual (false positive rate)    
\end{itemize}
Both functions are increasing in $s$, with $\pi_T(s) > \pi_N(s)$ for all $s > 0$ (the system performs better than random). The cost of 
surveillance is $C_G(s,t)$, decreasing in technology level $t$.

\textbf{Population Response.} Individuals choose adaptation effort $a_\theta \in [0, \bar{a}]$. As seen in the literature, adaptation includes avoiding monitored areas, 
limiting social connections, modifying communication patterns, and other behavioral changes that reduce detectability. For type $T$ individuals, 
adaptation reduces the probability of detection. For type $N$ individuals, adaptation may reduce false positive risk but imposes costs on daily 
life, such as the shrinking of their private spaces. Let $C_P(a)$ denote the cost of adaptation, increasing and convex in $a$.

\textbf{Probability bounds.} We interpret $\pi_T(\cdot)$ and $\pi_N(\cdot)$ as probabilities. To ensure they are always well-defined and lie 
in $[0,1]$, we restrict choices so that the argument of these functions never leaves their domain. Let $\pi_T,\pi_N:[0,\bar s]\to[0,1]$ be 
increasing and concave. We also bound adaptation to $a_\theta\in[0,1]$ for each type $\theta\in\{T,N\}$. Then for any feasible 
$(s,a_\theta)$ with $s\in[0,\bar s]$,
$$
s(1-a_\theta)\in[0,\bar s],
$$
so $\pi_T\!\big(s(1-a_T)\big)$ and $\pi_N\!\big(s(1-a_N)\big)$ are always valid probabilities. In this way, adaptation is modeled as a fractional 
reduction in ``effective surveillance,'' so it can scale detectability down but cannot make it negative or push the model outside the range 
where the probability functions are defined.

\subsection{Payoffs}

\textbf{Government Payoffs.} The government receives benefit $B$ from correctly identifying threats (preventing attacks, demonstrating control) 
and incurs cost $F$ from false positives (reputational damage, legal challenges, resistance to occupation). However, following the empirical 
observation that false positives may serve political functions in population control, we introduce parameter $\alpha \in [0,1]$ representing 
the political instrumentalization of errors. When $\alpha > 0$, the government derives partial benefit from false positives because they 
contribute to the general climate of fear and control.

Government expected payoff:
\begin{equation}
U_G = \lambda \cdot B \cdot \pi_T\!\big(s(1 - a_T)\big) + (1-\lambda) \cdot [\alpha B - (1-\alpha)F] \cdot \pi_N\!\big(s(1 - a_N)\big) - C_G(s,t)
\end{equation}
where $a_T$ and $a_N$ denote adaptation levels of threat and non-threat types respectively.  

\textbf{Population Payoffs.} Individuals face costs from being flagged (detention, harassment, restriction of movement) and from adaptation efforts. 
Type $T$ individuals additionally value successful evasion of detection. For simplicity, assume:
\begin{align}
U_T &= V_T\!\left(1 - \pi_T\!\big(s(1-a_T)\big)\right) - D \cdot \pi_T\!\big(s(1-a_T)\big) - C_P(a_T) \\
U_N &= - D \cdot \pi_N\!\big(s(1-a_N)\big) - C_P(a_N)
\end{align}
where $V_T$ is the value to type $T$ of evading detection and $D$ is the cost of being flagged by the system.

\subsection{Timing and Equilibrium Concept}

The game has two stages. In Stage 0, a surveillance technology level $t$ is deployed (taken as exogenous) and is publicly observed; it affects the cost of 
surveillance through $C_G(s,t)$. In Stage 1, given $t$, the government chooses surveillance intensity $s$ and individuals choose adaptation effort $a$ 
simultaneously; each individual observes their own type $\theta\in\{T,N\}$, but types are not publicly observed.

\begin{definition}[Bayesian Nash equilibrium conditional on technology]
Fix $t$. A Bayesian Nash equilibrium of the Stage 1 game induced by $t$ is a triple $(s^*(t),a_T^*(t),a_N^*(t))$ such that:
\begin{enumerate}[label=(\roman*), nosep]
    \item $s^*(t)$ maximizes the government's expected payoff given $(a_T^*(t),a_N^*(t))$;
    \item for each type $\theta\in\{T,N\}$, $a_\theta^*(t)$ maximizes that type's expected payoff given $s^*(t)$.
\end{enumerate}
\end{definition}

In what follows, we characterize the Stage 1 Bayesian Nash equilibrium for a given $t$ and derive comparative statics with respect to $t$ (and $\alpha$).

% 5. ANALYSIS
\section{Analysis}

The first-order conditions for optimal choices yield reaction functions. For the government, we set the function as follows:
\begin{equation}
    \frac{\partial U_G}{\partial s} 
    = \lambda B \,\pi'_T\!\big(s(1-a_T)\big)(1-a_T)
    + (1-\lambda)[\alpha B-(1-\alpha)F]\,\pi'_N\!\big(s(1-a_N)\big)(1-a_N)
    - \frac{\partial C_G}{\partial s} = 0    
\end{equation}

For type $T$ individuals:
\begin{equation}
    \frac{\partial U_T}{\partial a_T}
    = (V_T + D)\, s \,\pi'_T\!\big(s(1-a_T)\big) - C'_P(a_T) = 0
\end{equation}

For type $N$ individuals:
\begin{equation}
    \frac{\partial U_N}{\partial a_N}
    = D\, s \,\pi'_N\!\big(s(1-a_N)\big) - C'_P(a_N) = 0    
\end{equation}

\begin{proposition}[Existence and Uniqueness]
Under standard regularity conditions (compact strategy sets; $C_G$ and $C_P$ twice continuously differentiable and strictly convex; 
$\pi_T$ and $\pi_N$ continuous, increasing, and concave), a Bayesian Nash Equilibrium exists. In common parametric specifications, 
the equilibrium is unique and interior.
\end{proposition}

Existence follows from continuity of payoffs and compactness of the strategy spaces, which ensure a fixed point of best responses. 
The strategic interaction between surveillance and adaptation is driven by the fact that higher surveillance increases the stakes of being 
detected or falsely flagged, raising incentives to adapt, while adaptation reduces effective detectability through the argument $s(1-a_\theta)$ 
in $\pi_\theta(\cdot)$, shaping the government's optimal surveillance choice. 

\begin{proposition}[Technology Increases Surveillance]
    In equilibrium, surveillance intensity $s^*$ is increasing in technology level $t$ (holding the population cost function $C_P$ fixed).
\end{proposition}    

This extends \citet{dragu2021digital}'s core finding to the incomplete information setting. As technology reduces the cost of surveillance, the 
government optimally expands monitoring even when the population adapts in response. The strategic interaction between surveillance and adaptation 
creates an `arms race' dynamic, but the government consistently gains in equilibrium from a commitment/first-mover advantage in deploying $t$ 
(Stage 0), which lowers marginal surveillance cost in Stage 1.

\begin{proposition}[Error Instrumentalization Expands Surveillance]
Equilibrium surveillance $s^*$ is increasing in the instrumentalization parameter $\alpha$. As false positives become more politically valuable, 
the government expands surveillance beyond what would be optimal for pure security purposes.
\end{proposition}

Increasing $\alpha$ raises the marginal payoff weight on false positives in the government's objective, strengthening the incentive to increase 
$s$ because $\pi_N\!\big(s(1-a_N)\big)$ rises with surveillance. When $\alpha = 0$, the government faces a standard tradeoff between security 
benefits and false positive costs. But when $\alpha > 0$, false positives partially benefit the government by contributing to population control. 
In the limit as $\alpha \to 1$, every flagging, whether accurate or not, serves the government's interests. The comparative static result on 
$\alpha$ formalizes the observation from Hebron that algorithmic errors are not simply tolerated but may be functionally integrated into the 
surveillance system.

\begin{proposition}[Population Welfare Declines with Technology]
Expected utility of both type $T$ and type $N$ individuals is decreasing in technology level $t$ and in instrumentalization parameter $\alpha$.
\end{proposition}

A higher $t$ induces higher equilibrium surveillance, increasing both true-positive detection of type $T$ and false-positive flagging of type 
$N$, while also increasing incentives to invest in costly adaptation. A higher $\alpha$ further strengthens the government's incentive to expand 
surveillance even when it produces false positives, worsening expected outcomes for non-threats in particular. Under the model's assumptions, 
equilibrium welfare for both types weakly decreases with surveillance capability, and may decline strictly in typical cases: 
type $T$ individuals face higher detection probability, while type $N$ individuals face more frequent false positives and 
must expend more resources on adaptation. The burden falls disproportionately on non-threats, who gain nothing from successful evasion but bear 
costs of both flagging and adaptation. Moreover, when $\alpha > 0$, the welfare calculus becomes zero-sum between government and population. 
The government's political benefit from false positives is precisely the population's suffering from unjust flagging. This formalizes the sense 
in which algorithmic surveillance in contexts like Hebron constitutes what \citet{goodfriend2023algorithmic} terms `algorithmic state violence.'

% 6. DISCUSSION
\section{Discussion, Policy Implications, and Limitations}

The strategic interaction between surveillance and adaptation ensures that technological improvement does not reduce the intrusiveness of control 
but rather shifts its form. In equilibrium, as surveillance technology improves, the population adapts more intensively. This adaptation, including 
avoiding certain areas, limiting social connections, and modifying daily routines, represents a diffuse form of control that operates through self-discipline 
rather than direct coercion. Standard discussions of algorithmic governance treat errors as problems to be minimized through better data and improved 
algorithms. The analysis here suggests a different interpretation: when surveillance serves political functions beyond security, errors may become features 
that enhance rather than undermine the system's effectiveness. In our model, the government's net payoff weight on false positives is
$$
k(\alpha)\equiv \alpha B-(1-\alpha)F=\alpha(B+F)-F.
$$
This implies a critical threshold
$$
\alpha^*=\frac{F}{B+F}
$$
such that when $\alpha<\alpha^*$ false positives are net costly at the margin (the standard ``accuracy'' logic), while when $\alpha>\alpha^*$ 
false positives become net beneficial at the margin (an ``errors as control'' regime). This provides a clean political-economy restatement of 
the Hebron case: where reputational or legal constraints are weak (low $F$), the threshold $\alpha^*$ falls, making it easier for the system to 
rationally tolerate or even prefer ``messy'' outcomes. In such settings, the uncertainty produced by misclassification becomes a mechanism of 
generalized discipline: when civilians cannot predict whether they will be correctly or incorrectly classified, they must adapt their behavior 
as if they might always be flagged \citep{goodfriend2023algorithmic}. A related implication is that expanded surveillance can persist even when 
the prevalence of true threats is low. Even if $\lambda$ is small, a sufficiently large $\alpha$ (so that $k(\alpha)>0$) gives the government a 
continuing incentive to maintain or expand surveillance because control benefits arise from broad flagging of the population, not only from true positives. 
This formalizes the intuition that in contexts of occupation and population management, the logic of control can dominate the logic of security.

The model also clarifies which regulatory levers matter. International scrutiny, domestic judicial review, media attention, and other forms of 
oversight effectively raise the expected cost of false positives $F$ (or, equivalently, reduce the feasibility of instrumentalization by lowering 
the effective $\alpha$). The comparative prediction is straightforward: stronger accountability should reduce equilibrium surveillance intensity 
and reduce the tolerance for false positives, while weaker accountability permits higher $s^*$ and greater reliance on false positives as a tool 
of control. This connects naturally back to \citet{dragu2021digital}'s emphasis that institutional and legal constraints can be conceptualized as 
increasing the costs of preventive control. In this sense, the ``accuracy'' of the algorithm is not the central policy variable; the binding 
constraint is whether political and legal institutions make errors costly. 

A further implication is that the effective degree of instrumentalization need not be set only at the top of the hierarchy. The empirical record 
describes soldier photo quotas and competitive incentives for achieving `pairings' \citep{kubovich2022israeli}. This suggests a principal--agent 
extension: implementers may receive local benefits from flagging and data production (recognition, rewards, career incentives), effectively 
increasing the perceived benefit term associated with flagging (raising an implementer-level analogue of $\alpha$ or $B$) even when central 
authorities bear some reputational cost $F$. As a result, even if leadership prefers higher accuracy, street-level incentives can push the system 
toward a regime in which false positives are operationally convenient and politically tolerable. In this interpretation, ``errors as features'' 
can emerge endogenously from incentive design and institutional structure, not only from explicit political intent.

Our model also has some evident limitations. The first is that it is static, while the Blue Wolf context and the broader literature emphasize dynamic feedback 
loops and learning over time. A minimal dynamic extension would allow technology $t$ (or the performance parameters embedded in $\pi_T$ and $\pi_N$) 
to improve with data volume, and data volume to rise with surveillance intensity $s$. This might yield a self-reinforcing escalation of surveillance: 
higher $s$ generates more data (through increased encounters, images, and registrations), which improves operational capacity and lowers marginal costs, 
which then sustains or increases $s$ in the next period. This mechanism is consistent with the reported emphasis on data accumulation and the gamification 
of collection in Hebron \citep{kubovich2022israeli, goodfriend2023algorithmic}. Moreover, the model treats the population as atomistic individuals, 
abstracting from collective action and organized resistance. Extending the framework to incorporate coordination among surveilled populations would 
illuminate additional dynamics. 

% 7. POSSIBLE EXPANSION: Signaling Game Interpretation
\section{Possible Expansion: Signaling Game Interpretation}
A possible expansion of this model could be to reinterpret the Hebron setting as a signaling game, specifically as an adaptation of the classic 
Beer--Quiche game. In the traditional Beer--Quiche game, individuals of different types choose between two observable signals (beer or quiche) 
to convey their type to a receiver who then takes an action based on the observed signal. The sender's type is private information, and the 
receiver must infer the sender's type from the chosen signal. Within this framework, we can reinterpret the Hebron setting as a signaling 
environment in which Palestinians are the senders and IDF soldiers operating Blue Wolf are the receivers. Each Palestinian has a private 
type $\theta \in \{T, N\}$, where $T$ denotes a real security threat type (as defined by the classification system), and $N$ a civilian 
(non-threat). Crucially, unlike in the classic model, both types strictly prefer to be classified as ``civilian'' 
rather than ``security threat,'' because 
any threat classification leads to detention, harassment, or long-term consequences in the database. Observable signals include participation 
in protests, dense social networks, movement patterns, or other behaviors that are algorithmically encoded as risk factors. Soldiers, aided by 
Blue Wolf, map these signals into a binary action: classify and treat the individual as security threat (red/yellow) or civilian (green).

In a standard Beer--Quiche setup, equilibrium often hinges on one type having a differential taste for a costly signal (e.g., tough types liking 
beer more), which can sustain a separating equilibrium. Here, however, protest or politically salient behavior is costly for both types once it 
is surveilled, because the algorithm increases the probability of security threat classification regardless of $\theta$. 
This pushes the game toward pooling on low-risk signals, with both $T$ and $N$ types strategically avoiding protest or visible dissent. 
From the soldiers' perspective, 
the algorithm acts like an imprecise decoding rule that sometimes classifies $N$ as $T$. When such false positives are politically instrumental 
(high $\alpha$), the receiver is effectively rewarded for treating many signals as evidence of terrorism, which, as in the model we tackled in 
the previous sections, shrinks the range of behaviors that can safely function as ``civilian'' signals and reinforces a pooling equilibrium 
of widespread self-silencing among civilians.

% 8. CONCLUSION
\section{Conclusion}

This paper has developed a game-theoretic framework to analyze algorithmic surveillance as a strategic interaction between a state authority 
and a targeted population under incomplete information. Using Israel's Blue Wolf system in Hebron as an empirical case, we extended the 
preventive repression model of \citet{dragu2021digital} to incorporate classification errors and behavioral adaptation by surveilled individuals. 
Three main findings emerge from the analysis. First, technological improvement in surveillance capacity unambiguously increases equilibrium 
monitoring intensity, even when the population responds with costly adaptation. Second, when false positives carry political value for 
population control, captured by the instrumentalization parameter $\alpha$, the government rationally expands surveillance beyond what pure 
security objectives would warrant. Third, welfare losses from surveillance fall on both threat and non-threat types, but disproportionately 
burden ordinary civilians who bear the costs of flagging and adaptation without any offsetting benefit from evasion. 
The critical threshold $\alpha^* = F/(B+F)$ provides a simple diagnostic: where reputational or legal costs of errors are low, algorithmic 
systems can drift into an ``errors as features'' regime in which misclassification becomes functional rather than accidental. This reframes 
the policy problem. The challenge is not primarily technical, improving algorithmic accuracy, but institutional, ensuring that political and 
legal structures make errors costly. In contexts of occupation and weak accountability, the model predicts that surveillance will expand and 
persist even when the prevalence of genuine threats is low, because the logic of control dominates the logic of security. 
Several avenues for further research remain open. A dynamic extension could formalize how data accumulation and algorithmic learning create 
path-dependent escalation. Incorporating collective action among the surveilled population would illuminate the conditions under which 
coordinated resistance can shift equilibrium outcomes. 

% OPTIONAL APPENDIX 
\appendix
\section{Appendix: Notation and a Worked Illustration}

This appendix provides a short worked illustration using simple functional forms.

\subsection{Notation}
\begin{center}
\begin{tabular}{@{}ll@{}}
\toprule
Symbol & Meaning \\
\midrule
$s \in [0,\bar s]$ & surveillance intensity chosen by the government \\
$t$ & technology level (higher $t$ lowers marginal surveillance cost) \\
$\theta \in \{T,N\}$ & individual type: threat ($T$) or non-threat ($N$) \\
$\lambda$ & population share of type $T$ \\
$a_T,a_N \in [0,1]$ & adaptation effort by types $T$ and $N$ \\
$\pi_T(\cdot)$ & true-positive probability (detecting $T$ correctly) \\
$\pi_N(\cdot)$ & false-positive probability (flagging $N$ incorrectly) \\
$B$ & benefit from correctly detecting threats \\
$F$ & cost of false positives (reputation, legal constraints, backlash) \\
$\alpha \in [0,1]$ & degree to which false positives are politically instrumentalized \\
$k(\alpha)$ & net marginal weight on false positives: $k(\alpha)=\alpha B-(1-\alpha)F$ \\
$D$ & individual cost of being flagged \\
$V_T$ & type-$T$ value of evasion (avoiding detection) \\
$C_G(s,t)$ & government surveillance cost, decreasing in $t$ \\
$C_P(a)$ & adaptation cost, increasing and convex \\
\bottomrule
\end{tabular}
\end{center}

\subsection{A short illustration}
To keep the algebra simple, suppose that
\[
\pi_T(x)=x,\qquad \pi_N(x)=\eta x \ \ \text{with}\ \ \eta\in(0,1),\qquad
C_G(s,t)=\frac{s^2}{2t},\qquad C_P(a)=\frac{c}{2}a^2,
\]
and adaptation reduces ``effective surveillance'' as in the main text: the argument is $s(1-a_\theta)$.

Given $s$, the first-order conditions for individuals imply interior best responses
\[
a_T^*(s)=\frac{(V_T+D)s}{c},\qquad a_N^*(s)=\frac{D\eta s}{c}
\]
Substituting these into the government's objective yields a concave
quadratic in $s$, with interior optimum
\[
s^*(t,\alpha)=
\frac{\lambda B+(1-\lambda)\eta\,k(\alpha)}
{2\Big[\lambda B\frac{V_T+D}{c}+(1-\lambda)\eta\,k(\alpha)\frac{D\eta}{c}\Big]+\frac{1}{t}},
\qquad\text{where }k(\alpha)=\alpha(B+F)-F.
\]
This expression makes the paper's comparative statics transparent in a single instantiation:
holding other parameters fixed, higher $t$ lowers the $\frac{1}{t}$ term in the denominator, increasing $s^*$.
Likewise, higher $\alpha$ raises $k(\alpha)$, which (when the numerator is positive) increases the government's
incentive to surveil by making false positives less costly and potentially beneficial at the margin.

% REFERENCES
\newpage
\bibliographystyle{apalike}

\begin{thebibliography}{99}

\bibitem[Amnesty International, 2023]{amnesty2023automated}
Amnesty International. (2023).
\newblock \emph{Automated apartheid: How facial recognition fragments, segregates and controls Palestinians in the OPT}.
\newblock Amnesty International. Retrieved from \url{https://www.amnesty.org/en/documents/mde15/6701/2023/en/}

\bibitem[Abraham, 2024]{abraham2024lavender}
Abraham, Y. (2024, April 3).
\newblock `Lavender': The AI machine directing Israel's bombing spree in Gaza.
\newblock \emph{+972 Magazine}. Retrieved from \url{https://www.972mag.com/lavender-ai-israeli-army-gaza/}

\bibitem[Benjamin, 2019a]{benjamin2019race}
Benjamin, R. (2019).
\newblock \emph{Race after technology: Abolitionist tools for the new Jim Code}.
\newblock Polity Press.

\bibitem[Benjamin, 2019b]{benjamin2019playing}
Benjamin, G. (2019).
\newblock Playing at control: Writing surveillance in/for gamified society.
\newblock \emph{Surveillance \& Society, 17}(5), 699--713.

\bibitem[Browne, 2015]{browne2015dark}
Browne, S. (2015).
\newblock \emph{Dark matters: On the surveillance of Blackness}.
\newblock Duke University Press.

\bibitem[Byler, 2022]{byler2022terror}
Byler, D. (2022).
\newblock \emph{Terror capitalism: Uyghur dispossession and masculinity in a Chinese city}.
\newblock Duke University Press.

\bibitem[Dayan, 2022]{dayan2022dualstate}
Dayan, H. (2022).
\newblock Israel/Palestine: Authoritarian practices in the context of a dual state crisis.
\newblock In O.~Topak, M.~Mekouar, \& F.~Cavatorta (Eds.), \emph{New authoritarian practices in the Middle East and North Africa} (pp.~131--151).
\newblock Edinburgh University Press.

\bibitem[Dragu and Lupu, 2021]{dragu2021digital}
Dragu, T., \& Lupu, Y. (2021).
\newblock Digital authoritarianism and the future of human rights.
\newblock \emph{International Organization, 75}(4), 991--1017.

\bibitem[Dwoskin, 2021]{dwoskin2021israel}
Dwoskin, E. (2021, November 8).
\newblock Israel escalates surveillance of Palestinians with facial recognition program in West Bank.
\newblock \emph{The Washington Post}. Retrieved from \url{https://www.washingtonpost.com/world/middle_east/israel-palestinians-surveillance-facial-recognition/2021/11/05/3787bf42-26b2-11ec-8739-5cb6aba30a30_story.html}

\bibitem[Fatafta and Nashif, 2017]{fatafta2017surveillance}
Fatafta, M., \& Nashif, N. (2017, October 23).
\newblock Surveillance of Palestinians and the fight for digital rights (Policy brief).
\newblock \emph{Al-Shabaka: The Palestinian Policy Network}. Retrieved from \url{https://al-shabaka.org/briefs/surveillance-of-palestinians-and-the-fight-for-digital-rights/}

\bibitem[Gidron, 2023]{gidron2023crisis}
Gidron, N. (2023).
\newblock Why Israeli democracy is in crisis.
\newblock \emph{Journal of Democracy, 34}(3), 33--45.

\bibitem[Goodfriend, 2023]{goodfriend2023algorithmic}
Goodfriend, S. (2023).
\newblock Algorithmic state violence: Automated surveillance and Palestinian dispossession in Hebron's Old City.
\newblock \emph{International Journal of Middle East Studies, 55}, 461--478.

\bibitem[Harcourt, 2007]{harcourt2007against}
Harcourt, B. (2007).
\newblock \emph{Against prediction: Profiling, policing, and punishing in an actuarial age}.
\newblock University of Chicago Press.

\bibitem[Jamal, 2007]{jamal2007hollow}
Jamal, A. (2007).
\newblock Nationalizing states and the constitution of ``hollow citizenship'': Israel and its Palestinian citizens.
\newblock \emph{Ethnopolitics, 6}(4), 471--493.

\bibitem[Kubovich, 2022]{kubovich2022israeli}
Kubovich, Y. (2022, March 24).
\newblock Israeli troops' new quota: Add 50 Palestinians to tracking database every shift.
\newblock \emph{Haaretz}. Retrieved from \url{https://www.haaretz.com/israel-news/2022-03-24/ty-article/.premium/soldiers-not-allowed-off-shifts-until-they-enter-50-palestinian-names-in-database/00000180-5ba7-d97e-a7fb-7bf7361c0000}

\bibitem[Shalhoub-Kevorkian, 2015]{shalhoub2015security}
Shalhoub-Kevorkian, N. (2015).
\newblock \emph{Security theology, surveillance and the politics of fear}.
\newblock Cambridge University Press.

\bibitem[Smooha, 2002]{smooha2002ethnic}
Smooha, S. (2002).
\newblock The model of ethnic democracy: Israel as a Jewish and democratic state.
\newblock \emph{Nations and Nationalism, 8}(4), 475--503.

\bibitem[Rouhana, 1997]{rouhana1997identities}
Rouhana, N.~N. (1997).
\newblock \emph{Palestinian citizens in an ethnic Jewish state: Identities in conflict}.
\newblock Yale University Press.

\bibitem[Peled, 2008]{peled2008evolution}
Peled, Y. (2008).
\newblock The evolution of Israeli citizenship: An overview.
\newblock \emph{Citizenship Studies, 12}(3), 335--345.

\bibitem[Zureik, 2011]{zureik2011colonialism}
Zureik, E. (2011).
\newblock Colonialism, surveillance, and population control: Israel/Palestine.
\newblock In E.~Zureik, D.~Lyon, \& Y.~Abu-Laban (Eds.), \emph{Surveillance and control in Israel/Palestine}.
\newblock Routledge.

\end{thebibliography}
\end{document}